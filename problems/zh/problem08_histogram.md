# 第 8 题：并行计数 - 直方图计算

本题中我们探索新的一种并行计算模式：直方图计算。它涉及到原子操作（atomic operations），这是处理多个线程需要同时更新同一内存位置时产生冲突的重要工具。

## 🎯 学习目标

- 理解原子操作及其在解决并行冲突中的作用
- 学会设计多层聚合策略来构建可扩展的并行算法
- 掌握内存争用管理和性能优化技巧

## 直方图

直方图是统计学中最基础也是最重要的工具之一，它用来统计数据集中不同值出现的频率。当我们试图用并行计算来加速直方图计算时，会遇到数据竞争问题：多个线程可能会尝试同时增加同一个计数器，可能导致结果不正确。

设想一个简单场景：你正在统计一组数据中每个数值出现的次数。在单线程环境下，这非常简单：

```cpp
for (int i = 0; i < N; i++) {
    histogram[data[i]]++;
}
```

这段代码逻辑清晰正确，但处理大规模数据时效率不足。很自然地，我们会想到用 GPU 来并行化这个过程。于是我们写了一个简单的 CUDA 内核来处理数据：

```cpp
__global__ void histogram_kernel(int* data, int* histogram, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N) {
        int value = data[idx];
        histogram[value]++;
    }
}
```

表面看似合理，但 `histogram[value]++;` 这行简单代码实际上包含三个独立步骤：读取当前值、加一、写回新值。

问题在于：如果两个线程同时执行会怎样？

```
线程 A                   线程 B
t=histogram[value]=5
                        t=histogram[value]=5
t+=1
                        t+=1
histogram[value]=t=6
                        t=histogram[value]=t=6
```

上面的例子中，两个线程几乎同时读取了同一个计数器的值 5，然后各自增加 1 并写回。这就导致最终结果错误地变成了 6，一次增加操作丢失了。

这种现象称为竞争条件，它使得朴素的并行直方图计算变得不可靠。在GPU上，由于成千上万个线程同时运行，这种冲突概率大幅增加，可能导致完全错误的结果。

## 原子操作

解决竞争条件的关键是原子操作（atomic operations）。原子操作保证读取-修改-写入这个序列不会被其他线程中断，就像给这个操作加了一个不可分割的"保护罩"。

在CUDA中，我们可以使用 `atomicAdd` 函数来安全地增加计数器：

```cpp
atomicAdd(&histogram[value], 1);
```

这个函数确保在任何时刻，只有一个线程能够修改指定的内存位置。其他尝试访问同一位置的线程必须等待，直到当前操作完成。

CUDA提供了丰富的原子函数来满足不同的需求：

```cpp
atomicAdd(&address, value);    // address += value
atomicSub(&address, value);    // address -= value  
atomicMax(&address, value);    // address = max(address, value)
atomicMin(&address, value);    // address = min(address, value)
atomicCAS(&address, old, new); // Compare-and-swap
```

## 平衡性能与正确性

原子操作虽然解决了正确性问题，但我们需要考虑它们的性能。当许多线程竞争同一内存位置时，它们实际上会被迫串行执行，失去了并行计算的优势。

更重要的是，性能下降程度很大程度上取决于数据的分布特性。如果数据均匀分布在所有直方图桶（bins）中，那么冲突相对较少，性能还可以接受。但如果大部分数据都集中在少数几个桶中，那么这些热点桶就会成为严重的性能瓶颈，大量线程排队等待访问同一位置。

为了在保证正确性的同时最大化性能，我们可以使用“分层策略”。这个策略的核心思想是"分而治之"：先在局部解决问题，再合并全局结果。

第一层是块级别（block level）的处理。每个线程块使用共享内存构建自己的私有直方图。由于共享内存的访问速度比全局内存快得多，在共享内存上执行原子操作的代价相对较小。每个块内的线程协作处理分配给它们的数据，将结果累积到块的私有直方图中。

第二层是全局级别的合并。当所有块都完成了局部计算后，它们需要将自己的私有直方图合并到最终的全局直方图中。这时我们确实需要使用全局内存的原子操作，但关键的区别是，现在每个桶的原子操作次数大大减少了——从原来的"每个数据元素一次"变成了"每个块一次"。

这种策略就像组织一场大规模的投票统计：与其让所有选民都拥挤到一个投票箱前，不如在每个社区设立投票点，先统计本社区的结果，然后再将各个社区的结果汇总。这样既提高了效率，又保证了准确性。

通常情况下，block 数量越小，性能越好，因为共享内存比全局内存快得多。我们可以通过调整每个 block 的线程数来找到最佳平衡点。

## 代码实现

现在请你在 `student.cu` 中使用 CUDA 实现上述的分层直方图计算。输入数据均为 0-255 之间的整数。

## 总结

通过本题，你掌握了处理并行计算中内存冲突的关键技能。你现在理解了原子操作如何在并行代码中提供正确性保证，学会了设计多层聚合策略来最小化争用同时保持可扩展性，也体会到了不同同步方法之间的性能权衡。

你在这里学到的分层模式可以扩展到更大的问题规模，它构成了更复杂算法如并行哈希表和并发数据结构的基础。这种"局部聚合，全局合并"的思想是分布式计算和大数据处理中的核心概念。