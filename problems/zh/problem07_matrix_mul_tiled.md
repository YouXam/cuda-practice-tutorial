# 第 7 题：矩阵乘法

在本题中，我们将深入探索矩阵乘法的高性能实现，特别是在GPU上使用分块（Tiling）技术来优化计算。这个问题不仅是深度学习和科学计算的基石，也是现代人工智能革命的核心算法之一。

## 🎯 学习目标

- 理解矩阵乘法的基本概念和应用
- 掌握 GPU 上矩阵乘法的分块算法

## 矩阵乘法

让我们从矩阵乘法的基本概念开始。

矩阵乘法是线性代数中的一个基本操作，它将两个矩阵结合成一个新的矩阵。假设我们有两个矩阵 $A$ 和 $B$， $A$ 的维度是 $M \times K$， $B$ 的维度是 $K \times N$，那么它们的乘积 $C$ 将是一个 $M \times N$ 的矩阵。

矩阵乘法的计算规则是这样的： $C$ 的第 $i$ 行第 $j$ 列的元素是 $A$ 的第 $i$ 行和 $B$ 的第 $j$ 列对应元素的乘积之和，即：

$$
C_{i,j} = \sum_{k=0}^{K-1} A_{i,k} \cdot B_{k,j}
$$

举个例子，我们有一个 $2 \times 3$ 的矩阵 $A$ 和一个 $3 \times 2$ 的矩阵 $B$：

$$
A = \begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6
\end{bmatrix}, \quad
B = \begin{bmatrix}
7 & 8 \\
9 & 10 \\
11 & 12
\end{bmatrix}
$$

当我们计算结果矩阵 $C$ 的第一行第一列时，我们取 $A$ 的第一行 $[1, 2, 3]$ 和B的第一列 $[7, 9, 11]$，然后对应相乘再相加： $1 \times 7 + 2 \times 9 + 3 \times 11 = 58$。

继续这个过程，我们可以算出所有位置的值。 $A$ 的第一行和 $B$ 的第二列相乘得到 64， $A$ 的第二行和 $B$ 的第一列相乘得到 139，以此类推。最终我们得到一个 $2 \times 2$ 的结果矩阵 $C$：

$$
C = \begin{bmatrix}
58 & 64 \\
139 & 154
\end{bmatrix}
$$

这个看似简单的运算背后隐藏着巨大的应用价值。你在游戏中看到的每个3D物体都要经过矩阵变换来确定它在屏幕上的位置和角度。当 ChatGPT 生成一个回答时，它在内部进行着数百万次这样的矩阵运算来理解你的问题并组织语言。天气预报、药物研发、金融风险分析，这些看似不相关的领域都在大量使用矩阵乘法。

## 矩阵乘法的分块

当我们处理两个 $1024 \times 1024$ 的矩阵时，需要进行的运算次数是 $1024 \times 1024 \times 1024$，也就是超过 10 亿次乘法运算。如果用最朴素的方法来写代码，就像这样：

```cpp
for (int i = 0; i < M; i++) {           // 遍历A的每一行
    for (int j = 0; j < N; j++) {       // 遍历B的每一列
        float sum = 0;
        for (int k = 0; k < K; k++) {   // 计算点积
            sum += A[i][k] * B[k][j];
        }
        C[i][j] = sum;
    }
}
```

这个算法看起来很清晰，但它有一个致命的缺陷：内存访问效率极其低下。当我们计算结果矩阵 $C$ 的每一列时，都要重新读取矩阵 $A$ 的所有行。这意味着 $A$ 的每个元素都要被从内存中读取 $N$ 次。同样地，当我们计算 $C$ 的每一行时，都要重新读取矩阵 $B$ 的所有列，所以 $B$ 的每个元素也要被读取 $M$ 次。

这样的重复读取是巨大的浪费。在现代计算机中，从内存中读取数据的速度远远慢于处理器的计算速度。GPU 虽然有强大的计算能力，但如果大部分时间都在等待内存数据，这些计算资源就被白白浪费了。

解决这个问题的关键在于矩阵分块。我们不再一个元素一个元素地处理，而是把大矩阵切成一块一块的小矩阵，每次处理一对小块。这样做的好处是可以把这些小块加载到 GPU 的共享内存，然后反复使用它们进行计算，避免了重复的全局内存访问。

让我们用一个具体的例子来理解分块是如何工作的。假设我们要计算两个 $4 \times 4$ 矩阵的乘法，我们可以把它们各自分成四个 $2 \times 2$ 的小块：

```
矩阵A:                    矩阵B:
┌─────────┬─────────┐      ┌─────────┬─────────┐
│   A00   │   A01   │      │   B00   │   B01   │
│  (2×2)  │  (2×2)  │      │  (2×2)  │  (2×2)  │
├─────────┼─────────┤  ×   ├─────────┼─────────┤
│   A10   │   A11   │      │   B10   │   B11   │
│  (2×2)  │  (2×2)  │      │  (2×2)  │  (2×2)  │
└─────────┴─────────┘      └─────────┴─────────┘
```

我们可以像处理单个数字一样来处理这些块。结果矩阵 $C$ 的左上角块 $C_{0,0}$ 等于 $A_{0,0} \times B_{0,0}$ 加上 $A_{0,1} \times B_{1,0}$。

举例来说，

$$
A=\begin{bmatrix}
1 & 2 & 5 & 6 \\
3 & 4 & 7 & 8 \\
9 & 10 & 13 & 14 \\
11 & 12 & 15 & 16
\end{bmatrix}, \quad B=\begin{bmatrix}
1 & 0 & 1 & 1 \\
0 & 1 & 0 & 0 \\
1 & 1 & 0 & 1 \\
0 & 0 & 1 & 0
\end{bmatrix}
$$

则

$$C = A \times B = \begin{bmatrix} 6 & 7 & 7 & 6 \\
10 & 11 & 11 & 10 \\
22 & 23 & 23 & 22 \\
26 & 27 & 27 & 26 \end{bmatrix} $$

现在用分块的方式来计算 $C_{0,0}$：

$$
A_{00} = \begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}, \quad B_{00} = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
$$

那么 

$$A_{00} \times B_{00} = \begin{bmatrix} 1 & 2 \\
3 & 4 \end{bmatrix} \times \begin{bmatrix} 1 & 0 \\
0 & 1 \end{bmatrix} = \begin{bmatrix} 1 & 2 \\
3 & 4 \end{bmatrix}$$

$$A_{01} = \begin{bmatrix} 5 & 6 \\
7 & 8 \end{bmatrix}, \quad B_{10} = \begin{bmatrix} 1 & 1 \\
0 & 0 \end{bmatrix}$$

那么

$$A_{01} \times B_{10} = \begin{bmatrix} 5 & 6 \\
7 & 8 \end{bmatrix} \times \begin{bmatrix} 1 & 1 \\
0 & 0 \end{bmatrix} = \begin{bmatrix} 5 & 5 \\
7 & 7 \end{bmatrix}$$

最终：

$$C_{00} = A_{00} \times B_{00} + A_{01} \times B_{10} = \begin{bmatrix} 1 & 2 \\
3 & 4 \end{bmatrix} + \begin{bmatrix} 5 & 5 \\
7 & 7 \end{bmatrix} = \begin{bmatrix} 6 & 7 \\
10 & 11 \end{bmatrix}$$

这不是巧合，而是矩阵乘法的一个美妙性质：它在任何层级上都保持一致。

分块算法实际上并不能直接降低算法的时间复杂度，但它缓存局部性更好，将矩阵分块后，乘法可以局部处理小矩阵块，显著提高缓存命中率，减少内存访问延迟。

以 $16 \times 16$ 的块为例，当我们把 $A$ 的一个块加载到共享内存后，这个块中的每个元素都会被使用 16 次——对应 $B$ 块的 16 列，每列都需要用到 $A$ 块的这个元素。同样地， $B$ 块中的每个元素也会被使用 16 次，对应 $A$ 块的 16 行。这意味着我们用一次内存访问换来了 16 次计算，这种 16 倍的重用因子大大提高了计算效率。

更重要的是，这种重用模式将原本受内存带宽限制的算法转变为主要受计算能力限制的算法。再加上分块算法对并行化的友好性，我们可以在 GPU 上充分利用其大规模并行计算的优势。

现在，请你在 `student.cu` 中实现分块矩阵乘法算法。你需要使用 CUDA 的共享内存来存储矩阵块，并在每个线程块中计算对应的结果。

## 总结

通过这个题目，你不仅学会了矩阵乘法这个具体算法，还更深入地理解了如何分析算法的瓶颈，如何设计内存友好的数据访问模式，如何利用 GPU 的架构特性来最大化性能。

在接下来的第 8 题中，我们将探索一个完全不同但同样重要的并行计算挑战：原子操作和直方图计算。在那里，你将学会如何优雅地处理多个线程需要同时更新同一内存位置时产生的竞争条件。
